{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load obo file C:\\Users\\Daniel\\.pyppi\\go.obo\n",
      "C:\\Users\\Daniel\\.pyppi\\go.obo: fmt(1.2) rel(2017-07-14) 48,971 GO Terms\n",
      "load obo file C:\\Users\\Daniel\\.pyppi\\go.obo\n",
      "C:\\Users\\Daniel\\.pyppi\\go.obo: fmt(1.2) rel(2017-07-14) 48,971 GO Terms\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "This script runs the bootstrap kfold validation experiments as used in\n",
    "the publication.\n",
    "\n",
    "Usage:\n",
    "  validation.py [--interpro] [--pfam] [--mf] [--cc] [--bp]\n",
    "             [--use_cache] [--induce] [--verbose] [--abs] [--top=T]\n",
    "             [--model=M] [--n_jobs=J] [--n_splits=S] [--n_iterations=I]\n",
    "             [h_iterations=H] [--directory=DIR]\n",
    "  validation.py -h | --help\n",
    "\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --interpro    Use interpro domains in features.\n",
    "  --pfam        Use Pfam domains in features.\n",
    "  --mf          Use Molecular Function Gene Ontology in features.\n",
    "  --cc          Use Cellular Compartment Gene Ontology in features.\n",
    "  --bp          Use Biological Process Gene Ontology in features.\n",
    "  --induce      Use ULCA inducer over Gene Ontology.\n",
    "  --verbose     Print intermediate output for debugging.\n",
    "  --use_cache   Use cached features if available.\n",
    "  --abs         Take the absolute value of feature weights when computing top features.\n",
    "  --top=T       Top T features for each label to log [default: 25]\n",
    "  --model=M         A binary classifier from Scikit-Learn implementing fit,\n",
    "                    predict and predict_proba [default: LogisticRegression]\n",
    "  --n_jobs=J        Number of processes to run in parallel [default: 1]\n",
    "  --n_splits=S      Number of cross-validation splits [default: 5]\n",
    "  --h_iterations=H  Number of hyperparameter tuning iterations per fold [default: 60]\n",
    "  --n_iterations=I  Number of bootstrap iterations [default: 5]\n",
    "  --directory=DIR   Output directory [default: ./results/]\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "from pyppi.base import parse_args, su_make_dir\n",
    "from pyppi.data import load_network_from_path, load_ptm_labels\n",
    "from pyppi.data import testing_network_path, training_network_path\n",
    "from pyppi.data import load_go_dag, ipr_name_map, pfam_name_map\n",
    "\n",
    "from pyppi.models.binary_relevance import BinaryRelevance\n",
    "from pyppi.models import make_classifier\n",
    "from pyppi.model_selection.scoring import MultilabelScorer, Statistics\n",
    "from pyppi.model_selection.experiment import KFoldExperiment, Bootstrap\n",
    "from pyppi.model_selection.sampling import IterativeStratifiedKFold\n",
    "\n",
    "from pyppi.data_mining.features import AnnotationExtractor\n",
    "from pyppi.data_mining.uniprot import UniProt, get_active_instance\n",
    "from pyppi.data_mining.tools import xy_from_interaction_frame\n",
    "\n",
    "from pyppi.data_mining.ontology import id_to_node\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import (\n",
    "    recall_score, make_scorer, \n",
    "    label_ranking_average_precision_score,\n",
    "    label_ranking_loss,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "logging.captureWarnings(False)\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s', \n",
    "    datefmt='%m-%d-%Y %I:%M:%S',\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\n",
    "    'n_jobs': 12,\n",
    "    'n_splits': 5,\n",
    "    'n_iterations': 5,\n",
    "    'h_iterations': 60,\n",
    "    'induce': True,\n",
    "    'verbose': True,\n",
    "    'abs': True,\n",
    "    'top': 25,\n",
    "    'selection': [\n",
    "        UniProt.data_types().GO_MF.value,\n",
    "        UniProt.data_types().GO_BP.value,\n",
    "        UniProt.data_types().GO_CC.value,\n",
    "        UniProt.data_types().INTERPRO.value,\n",
    "        UniProt.data_types().PFAM.value\n",
    "    ],\n",
    "    'model': 'LogisticRegression',\n",
    "    'use_cache': True,\n",
    "    'directory': './results/'\n",
    "}\n",
    "n_jobs = args['n_jobs']\n",
    "n_splits = args['n_splits']\n",
    "n_iter = args['n_iterations']\n",
    "induce = args['induce']\n",
    "verbose = args['verbose']\n",
    "selection = args['selection']\n",
    "model = args['model']\n",
    "use_feature_cache = args['use_cache']\n",
    "direc = args['directory']\n",
    "hyperparam_iter = args['h_iterations']\n",
    "get_top_n = args['top']\n",
    "abs_weights = args['abs']\n",
    "\n",
    "backend = 'multiprocessing'\n",
    "go_dag = load_go_dag()\n",
    "\n",
    "# Set up the folder for each experiment run named after the current time\n",
    "folder = datetime.now().strftime(\"val_%y-%m-%d_%H-%M\")\n",
    "direc = \"{}/{}/\".format(direc, folder)\n",
    "su_make_dir(direc)\n",
    "json.dump(\n",
    "    args, fp=open(\"{}/settings.json\".format(direc), 'w'),\n",
    "    indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-20-2017 01:04:29] INFO: Loading training and testing data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First time loading on UniProt instance. Make take a few moments\n",
      "Warning: Loading dat files, may take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading training and testing data.\")\n",
    "ipr_map = ipr_name_map(lowercase_keys=False, short_names=False)\n",
    "pfam_map = pfam_name_map(lowercase_keys=False)\n",
    "uniprot = get_active_instance(\n",
    "    verbose=verbose,\n",
    "    sprot_cache=None,\n",
    "    trembl_cache=None\n",
    ")\n",
    "data_types = UniProt.data_types()\n",
    "labels = load_ptm_labels()\n",
    "annotation_ex = AnnotationExtractor(\n",
    "    induce=induce,\n",
    "    selection=selection,\n",
    "    n_jobs=n_jobs,\n",
    "    verbose=verbose,\n",
    "    cache=use_feature_cache,\n",
    "    backend='multiprocessing'\n",
    ")\n",
    "training = load_network_from_path(training_network_path)\n",
    "testing = load_network_from_path(testing_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08-20-2017 01:05:10] INFO: Preparing training and testing data.\n",
      "[08-20-2017 01:05:10] INFO: Computing class distributions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding new PPIs...\n",
      "Stringing selected features for each PPI...\n",
      "Finding new PPIs...\n",
      "Stringing selected features for each PPI...\n"
     ]
    }
   ],
   "source": [
    "# Get the features into X, and multilabel y indicator format\n",
    "logging.info(\"Preparing training and testing data.\")\n",
    "mlb = MultiLabelBinarizer(classes=labels)\n",
    "X_train_ppis, y_train = xy_from_interaction_frame(training)\n",
    "X_test_ppis, y_test = xy_from_interaction_frame(testing)\n",
    "mlb.fit(y_train)\n",
    "\n",
    "logging.info(\"Computing class distributions.\")\n",
    "json.dump(\n",
    "    Counter([l for ls in y_train for l in ls]), \n",
    "    fp=open(\"{}/training_distribution.json\".format(direc), 'w'),\n",
    "    indent=4, sort_keys=True\n",
    ")\n",
    "json.dump(\n",
    "    Counter([l for ls in y_test for l in ls]), \n",
    "    fp=open(\"{}/testing_distribution.json\".format(direc), 'w'),\n",
    "    indent=4, sort_keys=True\n",
    ")\n",
    "\n",
    "X_train = annotation_ex.transform(X_train_ppis)\n",
    "X_test = annotation_ex.transform(X_test_ppis)\n",
    "y_train = mlb.transform(y_train)\n",
    "y_test = mlb.transform(y_test)\n",
    "\n",
    "del annotation_ex\n",
    "del uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def fdr_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    return fp / (tp + fp)\n",
    "\n",
    "def get_coefs(clf):\n",
    "    \"\"\"\n",
    "    Return the feature weightings for each estimator. If estimator is a\n",
    "    pipeline, then it assumes the last step is the estimator.\n",
    "\n",
    "    :return: array-like, shape (n_classes_, n_features)\n",
    "    \"\"\"\n",
    "    def feature_imp(estimator):\n",
    "        if hasattr(estimator, 'steps'):\n",
    "            estimator = estimator.steps[-1][-1]\n",
    "        if hasattr(estimator, \"coef_\"):\n",
    "            return estimator.coef_\n",
    "        elif hasattr(estimator, \"coefs_\"):\n",
    "            return estimator.coefs_\n",
    "        elif hasattr(estimator, \"feature_importances_\"):\n",
    "            return estimator.feature_importances_\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Estimator {} doesn't support \"\n",
    "                \"feature coefficients.\".format(type(estimator)))\n",
    "\n",
    "    return feature_imp(clf.best_estimator_)\n",
    "\n",
    "\n",
    "def top_n_features(n, clf, absolute=False, vectorizer=None):\n",
    "    \"\"\"\n",
    "    Return the top N features. If clf is a pipeline, then it assumes\n",
    "    the first step is the vectoriser holding the feature names.\n",
    "\n",
    "    :return: array like, shape (n_estimators, n).\n",
    "        Each element in a list is a tuple (feature_idx, weight).\n",
    "    \"\"\"\n",
    "    top_features = []\n",
    "    coefs = get_coefs(clf)[0]\n",
    "\n",
    "    if absolute:\n",
    "        coefs = abs(coefs)\n",
    "    if hasattr(clf, 'steps') and vectorizer is None:\n",
    "        vectorizer = clf.steps[0][-1]\n",
    "    idx_coefs = sorted(\n",
    "        enumerate(coefs), key=itemgetter(1), reverse=True\n",
    "    )[:n]\n",
    "    if vectorizer:\n",
    "        idx = [idx for (idx, w) in idx_coefs]\n",
    "        ws = [w for (idx, w) in idx_coefs]\n",
    "        names = np.asarray(vectorizer.get_feature_names())[idx]\n",
    "        descriptions = np.asarray([get_term_name(go_dag, x) for x in names])\n",
    "        return list(zip(names, descriptions, ws))\n",
    "    else:\n",
    "        return [(idx, idx, coef) for (idx, coef) in idx_coefs]\n",
    "    \n",
    "def get_term_name(go_dag, term):\n",
    "    if 'go' in term.lower():\n",
    "        term = term.replace(\"go\", \"GO:\")\n",
    "        return go_dag[term.upper()].name\n",
    "    elif 'ipr' in term.lower():\n",
    "        return ipr_map[term.upper()]\n",
    "    elif 'pf' in term.lower():\n",
    "        return pfam_map[term.upper()]\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Setting up preliminaries and the statistics arrays\")\n",
    "logging.info(\"Found classes {}\".format(', '.join(mlb.classes)))\n",
    "n_classes = len(mlb.classes)\n",
    "seeds = range(n_iter)\n",
    "top_features = {l:{i:{j:[] for j in range(n_splits)} for i in range(n_iter)} for l in labels}\n",
    "param_distribution = {\n",
    "    'C': np.arange(0.01, 20.01, step=0.01),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "binary_scoring_funcs = [\n",
    "    ('Binary F1', f1_score) , \n",
    "    ('Precision', precision_score), \n",
    "    ('Recall', recall_score),\n",
    "    ('Specificity', specificity),\n",
    "    ('FDR', fdr_score)\n",
    "]\n",
    "multilabel_scores_funcs = [\n",
    "    ('Label Ranking Loss', label_ranking_loss), \n",
    "    ('Label Ranking Average Precision', label_ranking_average_precision_score), \n",
    "    ('Macro (weighted) F1', f1_score), \n",
    "    ('Macro (un-weighted) F1', f1_score)\n",
    "]\n",
    "n_scorers = len(binary_scoring_funcs)\n",
    "n_ml_scorers = len(multilabel_scores_funcs)\n",
    "\n",
    "# 2: position 0 is for validation, position 1 is for testing\n",
    "binary_statistics = np.zeros((n_iter, n_splits, n_classes, 2, n_scorers))\n",
    "multilabel_statistics = np.zeros((n_iter, n_splits, 2, n_ml_scorers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs_iter in range(n_iter):\n",
    "    logging.info(\"Fitting bootstrap iteration {}.\".format(bs_iter + 1))\n",
    "    cv = IterativeStratifiedKFold(n_splits=n_splits, random_state=seeds[bs_iter])\n",
    "    \n",
    "    for fold_iter, (train_idx, validation_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "        logging.info(\"Fitting fold iteration {}.\".format(fold_iter + 1))\n",
    "        y_valid_f_pred = []\n",
    "        y_test_f_pred = []\n",
    "        y_valid_f_proba = []\n",
    "        y_test_f_proba = []\n",
    "        \n",
    "        for label_idx, label in enumerate(labels):\n",
    "            logging.info(\"Fitting label {}.\".format(label))\n",
    "            \n",
    "            # Prepare all training and testing data\n",
    "            logging.info(\"Preparing data.\")\n",
    "            vectorizer = CountVectorizer(binary=False)\n",
    "            vectorizer.fit(X_train[train_idx])\n",
    "\n",
    "            X_train_l = vectorizer.transform(X_train[train_idx])\n",
    "            y_train_l = y_train[train_idx, label_idx]\n",
    "        \n",
    "            X_valid_l = vectorizer.transform(X_train[validation_idx])\n",
    "            y_valid_l = y_train[validation_idx, label_idx]\n",
    "\n",
    "            X_test_l = vectorizer.transform(X_test)\n",
    "            y_test_l = y_test[:, label_idx]\n",
    "            \n",
    "            # Build and fit classifier\n",
    "            logging.info(\"Fitting classifier.\")\n",
    "            clf = RandomizedSearchCV(\n",
    "                estimator=make_classifier(algorithm=model, random_state=0),\n",
    "                scoring='f1', cv=3, n_iter=hyperparam_iter, n_jobs=n_jobs, \n",
    "                refit=True, random_state=0, param_distributions=param_distribution,\n",
    "            )\n",
    "            clf.fit(X_train_l, y_train_l)\n",
    "            \n",
    "            # Validation scores in binary and probability format\n",
    "            y_valid_l_pred = clf.predict(X_valid_l)\n",
    "            y_valid_l_proba = clf.predict_proba(X_valid_l)\n",
    "            \n",
    "            # Held-out testing scores in binary and probability format\n",
    "            y_test_l_pred = clf.predict(X_test_l)\n",
    "            y_test_l_proba = clf.predict_proba(X_test_l)\n",
    "            \n",
    "            # Store these per label results in a list which we will\n",
    "            # later use to stack into a multi-label array.\n",
    "            y_valid_f_pred.append([[x] for x in y_valid_l_pred])\n",
    "            y_valid_f_proba.append([[x[1]] for x in y_valid_l_proba])\n",
    "            \n",
    "            y_test_f_pred.append([[x] for x in y_test_l_pred])\n",
    "            y_test_f_proba.append([[x[1]] for x in y_test_l_proba])\n",
    "            \n",
    "            # Perform scoring on the validation set and the external testing set.\n",
    "            logging.info(\"Computing fold label binary performance.\")\n",
    "            for func_idx, (func_name, func) in enumerate(binary_scoring_funcs):\n",
    "                if func_name in ['Specificity', 'FDR']:\n",
    "                    scores_v = func(y_valid_l, y_valid_l_pred)\n",
    "                    scores_t = func(y_test_l, y_test_l_pred)\n",
    "                else:\n",
    "                    scores_v = func(y_valid_l, y_valid_l_pred, average='binary')\n",
    "                    scores_t = func(y_test_l, y_test_l_pred, average='binary')\n",
    "                binary_statistics[bs_iter, fold_iter, label_idx, 0, func_idx] = scores_v\n",
    "                binary_statistics[bs_iter, fold_iter, label_idx, 1, func_idx] = scores_t\n",
    "                \n",
    "            logging.info(\"Computing top label features for fold.\")\n",
    "            # Get the top 20 features for this labels's run.\n",
    "            top_n = top_n_features(clf=clf, n=get_top_n, absolute=abs_weights, vectorizer=vectorizer)\n",
    "            top_features[label][bs_iter][fold_iter].extend(top_n)\n",
    "        \n",
    "        logging.info(\"Computing fold mult-label performance.\")\n",
    "        # True scores in multi-label indicator format\n",
    "        y_valid_f = y_train[validation_idx]\n",
    "        y_test_f = y_test\n",
    "        \n",
    "        # Validation scores in multi-label indicator format\n",
    "        y_valid_f_pred = np.hstack(y_valid_f_pred)\n",
    "        y_valid_f_proba = np.hstack(y_valid_f_proba)\n",
    "        \n",
    "        # Testing scores in multi-label probability format\n",
    "        y_test_f_pred = np.hstack(y_test_f_pred)\n",
    "        y_test_f_proba = np.hstack(y_test_f_proba)\n",
    "        \n",
    "        for func_idx, (func_name, func) in enumerate(multilabel_scores_funcs):\n",
    "            if func_name == 'Macro (weighted) F1':\n",
    "                scores_v = func(y_valid_f, y_valid_f_pred, average='weighted')\n",
    "                scores_t = func(y_test_f, y_test_f_pred, average='weighted')\n",
    "            elif func_name == 'Macro (un-weighted) F1':\n",
    "                scores_v = func(y_valid_f, y_valid_f_pred, average='macro')\n",
    "                scores_t = func(y_test_f, y_test_f_pred, average='macro')\n",
    "            elif func_name == 'Label Ranking Average Precision':\n",
    "                scores_v = func(y_valid_f, y_valid_f_proba)\n",
    "                scores_t = func(y_test_f, y_test_f_proba)\n",
    "            else:\n",
    "                scores_v = func(y_valid_f, y_valid_f_pred)\n",
    "                scores_t = func(y_test_f, y_test_f_pred)\n",
    "                \n",
    "            multilabel_statistics[bs_iter, fold_iter, 0, func_idx] = scores_v\n",
    "            multilabel_statistics[bs_iter, fold_iter, 1, func_idx] = scores_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Writing statistics to file.\")\n",
    "func_names = [n for n, _ in binary_scoring_funcs]\n",
    "iterables = [range(n_iter), range(n_splits), mlb.classes, [\"validation\", \"holdout\"], func_names]\n",
    "names=['bootstrap iteration', 'fold iteration', 'labels', 'condition', 'score function']\n",
    "index = pd.MultiIndex.from_product(iterables, names=names)\n",
    "binary_df = pd.DataFrame(binary_statistics.ravel(), index=index)[0]\n",
    "binary_df.to_csv('{}/{}'.format(direc, 'binary_stats.csv'), sep=',')\n",
    "np.save('{}/{}'.format(direc, 'binary_stats.np'), binary_statistics, allow_pickle=False)\n",
    "\n",
    "func_names = [n for n, _ in multilabel_scores_funcs]\n",
    "iterables = [range(n_iter), range(n_splits), [\"validation\", \"holdout\"], func_names]\n",
    "names=['bootstrap iteration', 'fold iteration', 'condition', 'score function']\n",
    "index = pd.MultiIndex.from_product(iterables, names=names)\n",
    "multilabel_df = pd.DataFrame(multilabel_statistics.ravel(), index=index)[0]\n",
    "multilabel_df.to_csv('{}/{}'.format(direc, 'multilabel_stats.csv'), sep=',')\n",
    "np.save('{}/{}'.format(direc, 'multilabel_stats.np'), multilabel_statistics, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Writing top features to file.\")\n",
    "json.dump(top_features, open('{}/{}'.format(direc, 'top_features.json'), 'wt'), indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyppi]",
   "language": "python",
   "name": "conda-env-pyppi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
